{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1ff950d",
   "metadata": {},
   "source": [
    "## Import the relevant libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b1388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import guidedlda\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538f9dbb",
   "metadata": {},
   "source": [
    "## Define function for preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text) # remove numbers\n",
    "    text = re.sub(r'et al\\.?', '', text) # remove \"et al.\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30fa2c",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04602d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('papers.csv')\n",
    "corpus = data['paper_text'].values.tolist()\n",
    "authors = pd.read_csv(\"authors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507099f2",
   "metadata": {},
   "source": [
    "## Fit the count vectorizer on the corpus by applying stopword removal and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=10000, stop_words='english', preprocessor=preprocess_text)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a449d7",
   "metadata": {},
   "source": [
    "## Fit the unguided LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f41fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = guidedlda.GuidedLDA(n_topics=5, n_iter=200, random_state=42, refresh=20)\n",
    "model.fit(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdbb032",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 50\n",
    "topic_word_unseeded = model.topic_word_\n",
    "topic_to_pick = 4\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word_unseeded):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ','.join(topic_words)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_unseeded_final =  np.array(vocab)[np.argsort(topic_word_unseeded[topic_to_pick])][:-(n_top_words+1):-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a9c3b",
   "metadata": {},
   "source": [
    "## Fit the topic model without seed keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_keywords = [[\"image processing\",\"convolutional neural\",\"deep convolutional\",\"object detection\",\"object recognition\",\"computer vision\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_guided = guidedlda.GuidedLDA(n_topics=5, n_iter=200, random_state=42, refresh=20)\n",
    "\n",
    "seed_topics = {}\n",
    "for st in seed_keywords:\n",
    "    for word in st:\n",
    "        seed_topics[vocab.index(word)] = 4\n",
    "\n",
    "        \n",
    "model_guided.fit(X.toarray(), seed_topics=seed_topics, seed_confidence=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38664fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 50\n",
    "topic_word_seeded = model_guided.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word_seeded):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ','.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_seeded_final =  np.array(vocab)[np.argsort(topic_word_seeded[topic_to_pick])][:-(n_top_words+1):-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af37e193",
   "metadata": {},
   "source": [
    "## Create word cloud for the unseeded topic 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a45f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"_\".join(x.split()) for x in topic_words_unseeded_final]\n",
    "\n",
    "word_priorities = {k:idx for idx,k in enumerate(text)}\n",
    "\n",
    "\n",
    "# Create a word cloud object\n",
    "wordcloud = WordCloud(width=2000, height=2000, background_color='white', min_font_size=10)\n",
    "\n",
    "wordcloud.generate_from_frequencies(word_priorities)\n",
    "\n",
    "plt.figure(figsize=(8,8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0de67c",
   "metadata": {},
   "source": [
    "## Create word cloud for the seeded topic 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ced405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your text\n",
    "\n",
    "text = [\"_\".join(x.split()) for x in topic_words_seeded_final]\n",
    "\n",
    "word_priorities = {k:idx for idx,k in enumerate(text)}\n",
    "\n",
    "\n",
    "# Create a word cloud object\n",
    "wordcloud = WordCloud(width=2000, height=2000, background_color='white', min_font_size=10)\n",
    "\n",
    "wordcloud.generate_from_frequencies(word_priorities)\n",
    "\n",
    "\n",
    "# Create a word cloud object\n",
    "wordcloud = WordCloud(width=1000, height=1000, background_color='white', min_font_size=10).generate(\",\".join(text))\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(8,8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342ee56",
   "metadata": {},
   "source": [
    "## Save the built models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ec7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(\"saved_models\")\n",
    "\n",
    "with open('saved_models/guidedlda_model.pickle', 'wb') as file_handle:\n",
    "    pickle.dump(model_guided, file_handle)\n",
    "    \n",
    "with open('saved_models/unguidedlda_model.pickle', 'wb') as file_handle:\n",
    "    pickle.dump(model, file_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7241368d",
   "metadata": {},
   "source": [
    "## OPTIONAL: Load the built model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # open a file, where you stored the pickled data\n",
    "# file = open('saved_models/guidedlda_model.pickle', 'rb')\n",
    "\n",
    "# # dump information to that file\n",
    "# guided_prev_version = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f53c9",
   "metadata": {},
   "source": [
    "## Plot papers picked across years for topic 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_to_paper = {}\n",
    "\n",
    "for index,paper in data.iterrows():\n",
    "        \n",
    "    if paper[\"year\"] not in year_to_paper:\n",
    "        year_to_paper[paper[\"year\"]] = [paper[\"paper_text\"]]\n",
    "    else:\n",
    "        year_to_paper[paper[\"year\"]].append(paper[\"paper_text\"])\n",
    "\n",
    "categories = {\"1987-1990\":[],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69361b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_dict = {\n",
    "    \"1987-1990\": list(range(1987, 1991)),\n",
    "    \"1991-1995\": list(range(1991, 1996)),\n",
    "    \"1996-2000\": list(range(1996, 2001)),\n",
    "    \"2001-2005\": list(range(2001, 2006)),\n",
    "    \"2006-2010\": list(range(2006, 2011)),\n",
    "    \"2011-2015\": list(range(2011, 2016)),\n",
    "    \"2016-2017\": list(range(2016, 2018))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeceda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_years_dict = {}\n",
    "\n",
    "# Loop through the items in the original dictionary and add them to the inverse dictionary\n",
    "for key, value in years_dict.items():\n",
    "    for year in value:\n",
    "        inv_years_dict[year] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_topics = model_guided.transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e31419",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_prominent_paper = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf10070",
   "metadata": {},
   "source": [
    "## Pick the top 5 papers across 5 year bins which have the highest topic score for CV by the guided topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35fb205",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction , df_item in tqdm(zip(predicted_topics, data.values.tolist())):\n",
    "    \n",
    "    year = df_item[1]\n",
    "    title = df_item[2]\n",
    "    \n",
    "    year_bucket = inv_years_dict[year]\n",
    "    \n",
    "    if year_bucket not in year_prominent_paper:\n",
    "        year_prominent_paper[year_bucket] = []\n",
    "        \n",
    "    year_prominent_paper[year_bucket].append({\"title\":title, \"cv_topic_score\":prediction[-1]})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf9aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the top 5 papers for each year\n",
    "top_papers = {}\n",
    "\n",
    "# Loop through each key in the dictionary\n",
    "for key in year_prominent_paper:\n",
    "    # Sort the list of dictionaries by cv_topic_score in descending order\n",
    "    sorted_list = sorted(year_prominent_paper[key], key=lambda x: x['cv_topic_score'], reverse=True)\n",
    "    # Get the top 5 titles for this key\n",
    "    top_titles = [d['title'] for d in sorted_list[:5]]\n",
    "    # Add the top 5 titles to the dictionary\n",
    "    top_papers[key] = top_titles\n",
    "\n",
    "# Define the column names for the CSV file\n",
    "fieldnames = list(top_papers.keys())\n",
    "\n",
    "# Write the rows to a CSV file\n",
    "with open('output.csv', mode='w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(fieldnames)\n",
    "    for i in range(5):\n",
    "        row = [top_papers[key][i] for key in fieldnames]\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
